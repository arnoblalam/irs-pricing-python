{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(logits):  \n",
    "    # logits is shape (S,) -> returns shape (S,)\n",
    "    # stable softmax possible, but for small S we can do direct\n",
    "    ex = np.exp(logits - np.max(logits))\n",
    "    return ex / np.sum(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def solve_dual_unconstrained(x, y, v, tol=1e-8, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Solve the dual (unconstrained) system for a maximum-entropy \n",
    "    binary choice + discrete noise model.\n",
    "    \n",
    "    x:  (n x K) matrix of covariates\n",
    "    y:  (n,)    binary labels in {0,1}\n",
    "    v:  (S,)    discrete noise support\n",
    "    \n",
    "    Returns:\n",
    "        beta_hat: shape (K,) the solution to F(beta)=0\n",
    "        p_hat:    shape (n,) p_i = logistic(x_i^T beta_hat)\n",
    "        w_hat:    shape (n,S) w_{i,s} = softmax( x_i^T beta_hat * v_s ), etc.\n",
    "    \"\"\"\n",
    "    n, K = x.shape\n",
    "    S = len(v)\n",
    "    xy = x * y[:,None]  # shape (n,K), just for convenience\n",
    "    sum_xy = np.sum(xy, axis=0)  # shape (K,)\n",
    "\n",
    "    def F_and_Jacobian(beta):\n",
    "        \"\"\"\n",
    "        Returns F(beta) = 0 (the residuals),\n",
    "        and optionally the Jacobian dF/dBeta (K x K).\n",
    "        \"\"\"\n",
    "        beta = beta.reshape(-1)  # ensure it's 1D\n",
    "        # We'll accumulate predictions to build F\n",
    "        # F_k(beta) = sum_i x_{i,k} y_i\n",
    "        #           - [ sum_i x_{i,k} p_i(beta) + sum_i sum_s x_{i,k} v_s w_{i,s}(beta) ]\n",
    "        \n",
    "        # 1) Compute p_i\n",
    "        xb = x @ beta  # shape (n,)\n",
    "        p = logistic(xb)  # shape (n,)\n",
    "\n",
    "        # 2) Compute w_{i,s} via softmax\n",
    "        #    w_{i,s} = exp( x_{i}^T beta * v_s ) / sum_r exp( x_{i}^T beta * v_r )\n",
    "        # We do this in a loop or vectorized. For clarity, we do a loop here.\n",
    "        w = np.zeros((n, S))\n",
    "        for i in range(n):\n",
    "            logits_i = xb[i] * v\n",
    "            w[i, :] = softmax(logits_i)\n",
    "\n",
    "        # 3) Build F_k\n",
    "        #    sum_i x_{i,k} y_i  - [ sum_i x_{i,k} p_i  + sum_i sum_s x_{i,k} v_s w_{i,s} ]\n",
    "        # We can do sum_i x_{i,k} p_i = x[:,k] dot p\n",
    "        # and sum_i sum_s x_{i,k} v_s w_{i,s} = x[:,k] dot [ sum_s v_s w_{i,s} ]\n",
    "        # We'll do it in a vector form\n",
    "        sum_xp = x.T @ p  # shape (K,)\n",
    "        sum_xvw = np.zeros(K)\n",
    "        for s_idx in range(S):\n",
    "            sum_xvw += x.T @ (v[s_idx] * w[:, s_idx])  # shape (K,)\n",
    "\n",
    "        F = sum_xy - (sum_xp + sum_xvw)  # shape (K,)\n",
    "\n",
    "        # For a proper Newton method, weâ€™d also compute the Jacobian = dF/dBeta.\n",
    "        # But implementing the full derivative of logistic + softmax in code can be a bit tedious.\n",
    "        # Let's do a simpler approach: let root() do a Jacobian-free method (like Broyden).\n",
    "        return F, None\n",
    "\n",
    "    # We'll solve F(beta)=0 with a zero initial guess or random guess\n",
    "    beta0 = np.zeros(K)  # or e.g. np.random.randn(K)\n",
    "    sol = root(lambda b: F_and_Jacobian(b)[0], beta0, method='hybr', tol=tol, options={'maxfev': maxiter})\n",
    "    \n",
    "    beta_hat = sol.x\n",
    "    # Now that we have beta, compute final p, w\n",
    "    xb = x @ beta_hat\n",
    "    p_hat = logistic(xb)\n",
    "    w_hat = np.zeros((n, S))\n",
    "    for i in range(n):\n",
    "        w_hat[i,:] = softmax(xb[i] * v)\n",
    "    \n",
    "    return beta_hat, p_hat, w_hat, sol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True The solution converged.\n",
      "Beta_hat: [ 0.22274545 -0.29993276]\n",
      "p_hat (first 5): [0.53795455 0.42249564 0.50451706 0.53035852 0.43356888]\n",
      "w_hat (first 5 rows):\n",
      " [[0.28410231 0.33077727 0.38512042]\n",
      " [0.44114846 0.3227392  0.23611234]\n",
      " [0.32732888 0.33329706 0.33937406]\n",
      " [0.29372321 0.33169687 0.37457991]\n",
      " [0.42528983 0.32553373 0.24917644]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 10000\n",
    "K = 2  # 2 covariates\n",
    "x_data = np.random.randn(n, K)\n",
    "# \"True\" process\n",
    "true_beta = np.array([1.5, -2.0])\n",
    "offset = 0.5\n",
    "# logistic part\n",
    "linear_part = offset + x_data @ true_beta\n",
    "prob_true = 1.0 / (1.0 + np.exp(-linear_part))\n",
    "y_data = (np.random.rand(n) < prob_true).astype(float)\n",
    "# noise support\n",
    "v_data = np.array([-1.0, 0.0, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True The solution converged.\n",
      "Beta_hat: [ 0.22 -0.3 ]\n",
      "p_hat (first 5): [0.53795455 0.42249564 0.50451706 0.53035852 0.43356888]\n",
      "w_hat (first 5 rows):\n",
      " [[0.28410231 0.33077727 0.38512042]\n",
      " [0.44114846 0.3227392  0.23611234]\n",
      " [0.32732888 0.33329706 0.33937406]\n",
      " [0.29372321 0.33169687 0.37457991]\n",
      " [0.42528983 0.32553373 0.24917644]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "beta_hat, p_hat, w_hat, sol = solve_dual_unconstrained(x_data, y_data, v_data)\n",
    "\n",
    "print(\"Converged:\", sol.success, sol.message)\n",
    "print(\"Beta_hat:\", np.round(beta_hat, 2))\n",
    "print(\"p_hat (first 5):\", p_hat[:5])\n",
    "print(\"w_hat (first 5 rows):\\n\", w_hat[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51, -2.04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "fit = model.fit(x_data, y_data)\n",
    "np.round(fit.coef_, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
